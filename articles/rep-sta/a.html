<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Article Title - Admir Selimovic</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Computer+Modern+Serif:wght@400;700&display=swap" rel="stylesheet">

    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$']]
          }
        };
    </script>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Get all figures to be numbered
        const figures = document.querySelectorAll('.auto-number-figure');
        
        // Number the figures
        figures.forEach((fig, index) => {
            const number = index + 1;
            fig.setAttribute('data-number', number);
        });
        
        // Update links that reference figures
        const links = document.querySelectorAll('a[href^="#fig-"]');
        links.forEach(link => {
            const targetId = link.getAttribute('href');
            const targetFigure = document.querySelector(targetId);
            if (targetFigure) {
                const figureNumber = targetFigure.getAttribute('data-number');
                link.innerHTML = `Figure ${figureNumber}`;
            }
        });
    });
    </script>
    
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const citations = document.querySelectorAll('.citation');
        citations.forEach((citation, index) => {
            const number = index + 1;
            citation.setAttribute('data-number', number);
        });
    
        const refs = document.querySelectorAll('a.reference');
        refs.forEach(ref => {
            const targetId = ref.getAttribute('href').slice(1); // Remove the '#'
            const targetCitation = document.getElementById(targetId);
            if (targetCitation) {
                const citationNumber = targetCitation.getAttribute('data-number');
                ref.innerHTML = `[${citationNumber}]`;
            }
        });
    });
    </script>

    <style>
        body {
            font-family: 'Roboto', Arial, sans-serif;
            margin: 0;
            padding: 0;
            counter-reset: figure-counter;
        }

        h1 {
            font-size: 2em; /* Change this according to your desired size if needed */
            margin: 0.67em 0; /* Default browser styles for h1 */
            font-weight: normal;
        }
        
        h1 a {
            text-decoration: none;
            color: #333; /* Set the desired color, this is black for instance */
            font-weight: normal;
            border: none;  /* Reset the default border */
            padding: 0;    /* Reset the default padding */
        }
        
        h1 a:hover {
            text-decoration: none;
            font-weight: normal;
        }
        
        .article-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px 20px 50px 20px;
        }

        .article-title {
            font-size: 32px;
            margin-top: 20px;
            margin-bottom: 20px;
        }

        .article-content {
            font-size: 22px; 
            counter-reset: figure-counter; /* Initializes the counter */
        }
        
        .article-content p {
            margin-bottom: 20px;
            text-align: justify;
            font-size: 22px;
        }

        .article-title, .article-content, .article-content h3, .article-content h4 {
            font-family: 'Computer Modern Serif', serif;
        }

        a {
            text-decoration: none;
            color: #333;
        }
    
        a[href^="#fig-"] {
            color: blue;
        }
    
        a:hover {
            text-decoration: underline;
        }
        
        figure {
            text-align: center; /* centers the content */
            margin: 20px 0; /* adds some margin on top and bottom */
            counter-increment: figure-counter;
        }
        
        figcaption {
            font-style: italic; /* makes the caption italic */
            font-size: 0.9em;  /* makes the caption slightly smaller than regular text */
            color: #666;       /* makes the caption a bit lighter than regular text */
            padding-top: 10px; /* adds some padding between image and caption */
            margin: 0 100px;
            text-align: left;
        }
        
        figcaption::before {
            content: "Figure " counter(figure-counter) ": ";
        }
        
        .article-content img {
            width: 100%;
            max-width: 100%;
        }

        .citation {
           font-size: 0.9em;
           margin-top: 20px;
           padding: 5px 0;
        }
        
        .reference {
           color: blue;
           text-decoration: none;
        }
        
    </style>
</head>

<body>
    <h1><a href="/"> Admir Selimovic</a></h1>

    <div class="article-container">
        <h2 class="article-title">Representation Stability under 3D Rotations in Equivariant and Nonequivariant MPNNs</h2>
        <div class="article-content">
            <p>

<div align="center"> 
  <img src="assets/rep-sta.gif">
</div>
                
<h3>Experiment: ModelNet10</h3>

We are processing undirected graphs $G = (V,E)$, consisting of nodes $v_i \in V$ and edges $e_{ij} \in E$. Nodes are assigned features $\mathbf{f}_i \in \mathbb{R}^{C_v}$ and edges are assigned attributes $\mathbf{a}_{ij} \in \mathbb{R}^{C_e}$, where $C_v$ and $C_e$ stand for the cardinality of features and attributes channel, respectively. The graph is embedded in $\mathbb{R}^3$; the nodes possess position vectors $\mathbf{x}_i \in \mathbf{X}$.


<a href="#gilmer2017" class="reference">[1]</a>
            
We follow the methods from \autocite{gilmer2017} (A) and \autocite{satorras2021} (B). The forward pass has two phases, a message passing phase and a readout phase.


\paragraph{Message passing}
Consider two nodes $v_i$ and $v_j$, their latent features $\mathbf{h}_i$ and $\mathbf{h}_j$, and the attributes $\mathbf{a}_{ij}$ of the edge that connects them. We update the node features by first computing a message $\mathbf{m}_{ij}$ from $v_j$ to $v_i$. 
The message function is expressed as $\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i, \mathbf{h}_j, \mathbf{a}_{i j}\right)$, whereby, in Method B, the message function is extended such that, aside the edge attribute, it involves the Euclidean distance: $\mathbf{m}_{ij}=\phi_m(\mathbf{h}_i, \mathbf{h}_j,||\mathbf{r}_{ij}||^2, a_{i j})$. However, for the purposes of this experiment, we will disregard the scalar attribute $a_{i j}$.

The message update function $\phi_m$  is directly parameterised by an MLP, i.e., it does not output a matrix that is used to linearly transform the node features but directly computes messages in a nonlinear way. Thus, the messages are nonlinear transformations conditioned on the edge attribute. Note that by conditioning the messages only on the Euclidean distance in Method B, $G$-mapping is guaranteed. 


Notably, Method B extends message passing such that also the node positions become updatable: 
\begin{equation} \label{eq:egnn-node-pos-update}
\mathbf{x}^{\prime}_i=\mathbf{x}_i + \sum_{j\in N_i}(\mathbf{r}_{ij})\phi_p(\mathbf{m}_{ij})
\end{equation} 
The position function $\phi_p$ gives a scalar which weighs the amount by which $\mathbf{x}_i$ is translated in the direction of the neighbors or in the opposite direction. This is done in a $G$-equivariant way. 
Since the position function $\phi_p$ is conditioned on the distance between the node and its neighbors, $\sum_{j \in N} (\mathbf{r}_{ij})$, it is invariant; i.e., since the function gives a scalar, the attributes are invariant. 
%Recall that scalar fields are contrasted with spectral feature fields, where attributes can be $G$-equivariant. 
However, the method learns the weights which determine the amount of influence nodes have on each other. Due to this, it allows for more adaptation and flexibility than the basic message passing. 

While both Method A and Method B preserve preserve permutation $G$-mapping on the set of nodes $V$ in the same way as standard GNNs do, Method B preserves rotation and translation $G$-mapping on the set of coordinates $\mathbf{x}_i$. Equation \ref{eq:egnn-node-pos-update} is the main difference of this method compared to standard GNNs and it is the reason why translation, rotation, and reflection $G$-mapping is preserved.
However still, the messages are obtained via invariant attributes $\|\mathbf{r}_{ij}\|^2$, which are limiting despite ensuring a $G$-map.

The next step in both methods is to aggregate the computed messages via a permutation-invariant function. Permutation invariance is needed because the way the messages are indexed, just like the way nodes are indexed, meaning the order, should not influence the results. We use sum pooling. The aggregated message $\mathbf{m}_i = \sum_{j \in N_i} \mathbf{m}_{i j}$ is a vector that is used to obtain an updated node representation
$\mathbf{h}_i^{\prime}=\phi_h \left(\mathbf{h}_i, \sum_{j \in N_i} \mathbf{m}_{i j}\right)$, where $\phi_h$ is a node update function imlemented as an MLP, whose input are the current node features $\mathbf{h}_i$ and the aggregated message $\mathbf{m}_i$.



\paragraph{Readout}
We perform a graph-level classification task on the dataset. To obtain a graph-level embedding, we apply a permutation-invariant aggregation of all the nodes' features: $\hat{y} = \sum_{v \in V}(\{\mathbf{h}^T_v\})$, where $T$ denotes the step after the last message passing layer.


\subsubsection*{Dataset and input representation}


ModelNet10 dataset \autocite{wu2015} contains 4,899 3D models (meshes) representing an object from 10 classes. We processed the meshes to form graphs embedded in $\mathbb{R}^3$. 
%We normalised mesh vertex positions to $\{-1,1\}$. 
Nodes form an $8\times8\times8$ regular grid. The edges are inferred by connecting nodes within a pre-set maximum radius value. The nodes are assigned one-dimensional feature vectors, representing the Euclidean distance to the closest point on the surface of the initial mesh.

The three variants of Method A pose different demands on edge attributes. In the first, edges have no attributes. In the second, Euclidean distance between connected nodes forms attributes. In the third, attributes are formed by computing relative Cartesian coordinates of connected nodes.

Each graph is assigned a one-dimensional feature vector representing the object class.

We sampled 3.991 graphs for training and 908 for validation.

\subsubsection*{Training}
 Batch size is set to 64; the number of message-passing layers to 2; the number of hidden features to 256; and the learning rate to $3e^{-4}$. Number of epochs is set to 50. SGD uses the ADAM optimiser. Parameter total is 923 K (Method A) and 1.1 M (Method B).



\subsubsection*{Results}


Method A that does not use edge attributes ($\mathbf{a}_{ij} = \emptyset$) and therefore, it disregards any information the edges may carry, beyond connectivity. It is ignorant of the geometric structure between the nodes and its message passing scheme utilises the node feature vectors. Applying different rotations to the input graph gives the same latent representations. 

Another model following the Method A has the geometric quantity of Euclidean distance injected into the edge attributes, $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$.
%The result is identical to the first model. 
The representations learned with this rotation-invariant model do not change under the rotation of the input representations. 
%This implies that the model would not distinguish an object from its rotated instance during classification. 
Rotation invariance stems from the norm in the edge attribute definition 
%$\mathbf{a}_{ij} = \|\mathbf{x}_j - \mathbf{x}_i\|$ 
being invariant to rotations. 

Last model following the Method A has relative Cartesian coordinates as attributes, $\mathbf{a}_{ij} = \mathbf{r}_{ij}$. Here, latent representations show instability under rotation. The definition of edge attribute here as the distance between two nodes, $\mathbf{r}_{ij}$, does guarantee translation but not rotation $G$-mapping. See <a href="#fig-rep-sta-1">Figure</a> for a visualisation.

<figure id="fig-rep-sta-1" class="auto-number-figure">
    <img src="assets/rep-sta-1.png" width="600">
    <figcaption>
        Hidden representations of the model before pooling, using relative Cartesian coordinates as edge attributes. The disk radius reflects the first node feature value. For the input representation, 1,500 points per 3D model randomly sampled from the embedding space $\mathbb{R}^3$. Two scenarios are shown: (a) with an applied rotation of $ \theta = 0^{\circ} $ and (b) with $ \theta = 180^{\circ} $. The input representation for this visualisation differs from the others in the experiment solely in the pre-processing involving random sampling.
    </figcaption>
</figure>

Next, we compare Method B with the three variants of Method A, presented in the Table \ref{tab:egnn-mpnn-compar}. Method B is outperformed only by Method A variant with relative Cartesian coordinates as edge attributes. However, in the second part of the experiment, where we apply a random rotation to the input, its accuracy decreases significantly. This is expected, since relative Cartesian coordinates are not invariant to rotation. 


The other two variants of Method A (no edge attributes and Euclidean distance) fall behind all the others in the first part of the experiment. Since they do not involve information on the direction that the messages come from, the accuracy is unchanged with the transformed input; they are invariant to rotation. Finally, we observe the stability under rotation with Method B and the higher accuracy relative to the rotation-invariant models.


\begin{table}[H]
\begin{center}
\begin{tabular}{lccc}
\hline 
Method & ACC(a) & ACC(b) \\
\hline 
A ($\mathbf{a}_{ij} = \emptyset$) & .6398 & .6398 \\
A ($\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$) & .6861 & .6861 \\
A ($\mathbf{a}_{ij} = \mathbf{r}_{ij}$) & .8061 & .3876 \\
B ($\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2, \phi_p$) & .7643 & .7643 \\
\hline
\end{tabular}
\caption{Comparison of feature stability under rotation accross message passing model variants. ACC (a): validation accuracy; ACC (b): Random rotation test accuracy} \label{tab:egnn-mpnn-compar}
\end{center}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{img/egnn-valid-acc-comparison.png}
\caption{Training performance comparison. Method A with edge attributes $\mathbf{a}_{ij} = \emptyset$, $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$, and $\mathbf{a}_{ij} = \mathbf{r}_{ij}$. Method B with edge attributes $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$ and node position update function $\phi_p$.}
\end{figure}


        </div>
    </div>

<div class="references-section">
   <h2>References</h2>
   <div class="citation" id="gilmer2017">
      Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl. 
      <a href="http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf">"Neural Message Passing for Quantum Chemistry"</a>. 
      Proceedings of the 34th International Conference on Machine Learning, 2017. pp. 1263-1272.
   </div>
   <!-- ... Other citations ... -->
</div>
    
</body>

</html>
