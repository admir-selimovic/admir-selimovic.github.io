<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Representation Stability under 3D Rotation in Message Passing Neural Networks - Admir Selimovic</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Computer+Modern+Serif:wght@400;700&display=swap" rel="stylesheet">

    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                tags: 'ams',
                packages: {'[+]': ['ams']}
            },
            startup: {
                ready: function () {
                    MathJax.startup.defaultReady();
                    MathJax.startup.promise.then(function () {
                        // Adjust styles here, if needed
                    });
                }
            }
        };
    </script>

    
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Auto-numbering for figures
        const figures = document.querySelectorAll('.auto-number-figure');
        figures.forEach((fig, index) => {
            const number = index + 1;
            fig.setAttribute('data-number', number);
        });
    
        // Auto-numbering for tables
        const tables = document.querySelectorAll('table');
        tables.forEach((table, index) => {
            const number = index + 1;
            table.setAttribute('data-number', number);
        });
    
        const links = document.querySelectorAll('a[href^="#fig-"], a[href^="#tab-"]');
        links.forEach(link => {
            const targetId = link.getAttribute('href');
            const targetItem = document.querySelector(targetId);
            if (targetItem) {
                const itemNumber = targetItem.getAttribute('data-number');
                if (targetId.startsWith('#fig-')) {
                    link.innerHTML = `Figure ${itemNumber}`;
                    link.style.color = 'blue';
                } else if (targetId.startsWith('#tab-')) {
                    link.innerHTML = `Table ${itemNumber}`;
                    link.style.color = 'blue';
                }
            }
        });
    
        // Update citations in the text
        const citationPlaceholders = document.querySelectorAll('.autocite');
        citationPlaceholders.forEach(placeholder => {
            const refId = placeholder.getAttribute('data-ref');
            const targetCitation = document.querySelector(`#${refId}`);
            if (targetCitation) {
                const authors = targetCitation.getAttribute('data-authors').split(',');
                const year = targetCitation.getAttribute('data-year');
                let citationText = `<a href="#${refId}" class="citation-link">(`;
                if (authors.length === 1) {
                    citationText += `${authors[0]} ${year}`;
                } else {
                    citationText += `${authors[0]} et al. ${year}`;
                }
                citationText += `)</a>`;
                placeholder.innerHTML = citationText;
            }
        });
    });

    // Assign numbers to equations
    let equations = document.querySelectorAll(".equation");
    equations.forEach((equation, index) => {
        equation.querySelector(".eq-number").innerText = `( ${index + 1} )`;
    });
    </script>
    
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const content = document.querySelector('.content');
        if (content) {
            const paragraphs = content.innerHTML.split(/\n\s*\n/); // Split content at empty lines
            content.innerHTML = paragraphs.map(p => `<p>${p.trim()}</p>`).join(''); // Wrap each split content in <p> tags
        }
    });
    </script>

    <style>
        body {
            font-family: 'Roboto', Arial, sans-serif;
            margin: 0;
            padding: 0;
            counter-reset: figure-counter;
            counter-reset: table-counter;
        }

        h1 {
            font-size: 1.5em; /* Change this according to your desired size if needed */
            margin: 0.67em 0; /* Default browser styles for h1 */
            font-weight: normal;
        }
        
        h1 a {
            text-decoration: none;
            color: #333; /* Set the desired color, this is black for instance */
            font-weight: normal;
            border: none;  /* Reset the default border */
            padding: 0;    /* Reset the default padding */
        }
        
        h1 a:hover {
            text-decoration: none;
            font-weight: normal;
        }
        
        .article-container {
            max-width: 750px; /* Make this consistent with the article-title max-width */
            margin: 0 auto; /* This will center the container */
            padding: 20px 20px 50px 0; /* Add some right padding to avoid touching the screen's right edge */
        }

        .article-title {
            max-width: 800px; /* Adjust this value if you want the title to break into more or fewer lines */
            margin: 5% 5% 0 20%; 
            font-size: 38px;
            text-align: left; /* Align the title to the left */
            padding: 0 20px 0 0; /* Add padding only to the right to avoid touching the screen's right edge */
        }

        .article-content {
            font-size: 22px; 
            counter-reset: figure-counter; /* Initializes the counter */
        }
        
        .article-content p {
            margin-bottom: 20px;
            text-align: justify;
            font-size: 22px;
        }

        .article-content, .article-content h3, .article-content h4 {
            font-family: 'Computer Modern Serif', serif;
        }

        a {
            text-decoration: none;
            color: #333;
        }
    
        a[href^="#fig-"] {
            color: blue;
        }
    
        a:hover {
            text-decoration: underline;
        }
        
        figure {
            text-align: center; /* centers the content */
            margin: 20px 0; /* adds some margin on top and bottom */
            counter-increment: figure-counter;
        }
        
        figcaption {
            font-style: italic;
            font-size: 0.8em;  /* makes the caption slightly smaller than regular text */
            color: #000000;       /* makes the caption a bit lighter than regular text */
            padding-top: 10px; /* adds some padding between image and caption */
            margin: 0 100px;
            text-align: left;
        }
        
        figcaption::before {
            content: "Figure " counter(figure-counter) ": ";
        }

        table {
            counter-increment: table-counter;
            font-size: 0.8em;
            width: 70%;
            margin: auto;
            border-collapse: collapse;  /* Ensures borders don't double up */
        }

        th {
            text-align: left;
            padding: 8px;
        }
        
        td {
            text-align: left;
            padding: 8px;
        }
        
        /* Thick line above the header */
        thead tr:first-child {
            border-top: 1pt solid black;
        }
        
        /* Thin line below the header */
        thead tr:last-child {
            border-bottom: 1px solid black;
        }
        
        /* Thick line at the bottom of the table */
        tbody tr:last-child {
            border-bottom: 1pt solid black;
        }

        table + figcaption::before {
            content: "Table " counter(table-counter) ": ";
        }
        
        .article-content img {
            width: 100%;
            max-width: 100%;
        }

        .citation {
           font-size: 0.9em;
           margin-top: 20px;
           padding: 5px 0;
        }

        .citation a {
            color: blue;
        }
        
        .citation-link {
            text-decoration: none;
            color: blue;
        }
        
        .citation-link:hover {
            text-decoration: underline;
        }
        
        .reference {
           color: blue;
           text-decoration: none;
        }

        .references-section {
           margin-top: 40px;
           border-top: 1px solid #ccc;
           padding-top: 20px;
        }
        
        .references-section h2 {
           font-size: 1.5em;
           margin-bottom: 20px;
        }        

        .content p {
            margin-bottom: 1em; // or any other styling you want for paragraphs
        }

        .equation {
            text-align: center;
            position: relative;
        }
        
        .eq-number {
            position: absolute;
            right: 0;
            top: 50%;
            transform: translateY(-50%);
        }
        
    </style>
</head>

<body>
    <h1><a href="/">&nbsp;&nbsp;Admir Selimovic Research Repository</a></h1>

    <div class="content">
        
    <h2 class="article-title">Representation Stability under 3D Rotation in Message Passing Neural Networks</h2>

    <div class="article-container">
        <div class="article-content">
        
        <div align="center"> 
          <img src="assets/rep-sta.gif">
        </div>

        <h3>Summary</h3>

        This experiment examines the representation stability of graph neural networks when subjected to 3D rotation of input data. 
        Focusing on two leading methods, E(n) Equivariant Graph Neural Networks and Message Passing Neural Networks, we aim to discern their mechanisms in updating node features of graphs embedded in 3D space. 
        Using undirected graphs from the ModelNet10 dataset, the study offers a comprehensive analysis of each method's resilience and efficiency in retaining essential information amid 3D rotational perturbations. 
        The conclusions underscore the importance of integrating geometric relationships into edge attributes for improved representation stability.
            
        <h3>Introduction</h3>
        
        We are processing undirected graphs $G = (V,E)$, consisting of nodes $v_i \in V$ and edges $e_{ij} \in E$. Nodes are assigned features $\mathbf{f}_i \in \mathbb{R}^{C_v}$ and edges are assigned attributes $\mathbf{a}_{ij} \in \mathbb{R}^{C_e}$, where $C_v$ and $C_e$ stand for the cardinality of features and attributes channel, respectively. The graph is embedded in $\mathbb{R}^3$; the nodes possess position vectors $\mathbf{x}_i \in \mathbf{X}$.
                    
        We follow the methods from <span class="autocite" data-ref="gilmer2017"></span> (A) and <span class="autocite" data-ref="satorras2021"></span> (B). The forward pass has two phases, a message passing phase and a readout phase.
        
    
        <h3>Message passing</h3>
        Consider two nodes $v_i$ and $v_j$, their latent features $\mathbf{h}_i$ and $\mathbf{h}_j$, and the attributes $\mathbf{a}_{ij}$ of the edge that connects them. We update the node features by first computing a message $\mathbf{m}_{ij}$ from $v_j$ to $v_i$. 
        The message function is expressed as: 
        $$\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i, \mathbf{h}_j, \mathbf{a}_{i j}\right) ,$$ 
        whereby, in Method B, the message function is extended such that, aside the edge attribute, it involves the Euclidean distance: 
        $$\mathbf{m}_{ij}=\phi_m(\mathbf{h}_i, \mathbf{h}_j,||\mathbf{r}_{ij}||^2, a_{i j}).$$
        However, for the purposes of this experiment, we will disregard the scalar attribute $a_{i j}$.
        
        The message update function $\phi_m$  is directly parameterised by an MLP, i.e., it does not output a matrix that is used to linearly transform the node features but directly computes messages in a nonlinear way. Thus, the messages are nonlinear transformations conditioned on the edge attribute. Note that by conditioning the messages only on the Euclidean distance in Method B, $G$-mapping is guaranteed. 
            
        Notably, Method B extends message passing such that also the node positions become updatable: 
        \begin{equation} \label{eq:egnn-node-pos-update}
        \mathbf{x}^{\prime}_i=\mathbf{x}_i + \sum_{j\in N_i}(\mathbf{r}_{ij})\phi_p(\mathbf{m}_{ij})
        \end{equation} 
        The position function $\phi_p$ gives a scalar which weighs the amount by which $\mathbf{x}_i$ is translated in the direction of the neighbors or in the opposite direction. This is done in a $G$-equivariant way. 
        Since the position function $\phi_p$ is conditioned on the distance between the node and its neighbors, $\sum_{j \in N} (\mathbf{r}_{ij})$, it is invariant; i.e., since the function gives a scalar, the attributes are invariant. 
        However, the method learns the weights which determine the amount of influence nodes have on each other. Due to this, it allows for more adaptation and flexibility than the basic message passing. 
        
        While both Method A and Method B preserve preserve permutation $G$-mapping on the set of nodes $V$ in the same way as standard GNNs do, Method B preserves rotation and translation $G$-mapping on the set of coordinates $\mathbf{x}_i$. Equation \ref{eq:egnn-node-pos-update} is the main difference of this method compared to standard GNNs and it is the reason why translation, rotation, and reflection $G$-mapping is preserved.
        However still, the messages are obtained via invariant attributes $\|\mathbf{r}_{ij}\|^2$, which are limiting despite ensuring a $G$-map.
        
        The next step in both methods is to aggregate the computed messages via a permutation-invariant function. Permutation invariance is needed because the way the messages are indexed, just like the way nodes are indexed, meaning the order, should not influence the results. We use sum pooling. The aggregated message 
        $\mathbf{m}_i = \sum_{j \in N_i} \mathbf{m}_{i j}$ 
        is a vector that is used to obtain an updated node representation:
        $$\mathbf{h}_i^{\prime}=\phi_h \left(\mathbf{h}_i, \sum_{j \in N_i} \mathbf{m}_{i j}\right) ,$$
        where $\phi_h$ is a node update function imlemented as an MLP, whose input are the current node features $\mathbf{h}_i$ and the aggregated message $\mathbf{m}_i$.
        
        
        <h3>Readout</h3>
        
        We perform a graph-level classification task on the dataset. To obtain a graph-level embedding, we apply a permutation-invariant aggregation of all the nodes' features: 
        $$\hat{y} = \sum_{v \in V}(\{\mathbf{h}^T_v\}) ,$$
        where $T$ denotes the step after the last message passing layer.

            
        <h3>Dataset and input representation</h3>
        
        ModelNet10 dataset <span class="autocite" data-ref="wu2015"></span> contains 4,899 3D models (meshes) representing an object from 10 classes. We processed the meshes to form graphs embedded in $\mathbb{R}^3$. 
        Nodes form an $8\times8\times8$ regular grid. The edges are inferred by connecting nodes within a pre-set maximum radius value. The nodes are assigned one-dimensional feature vectors, representing the Euclidean distance to the closest point on the surface of the initial mesh.
        
        The three variants of Method A pose different demands on edge attributes. In the first, edges have no attributes. In the second, Euclidean distance between connected nodes forms attributes. In the third, attributes are formed by computing relative Cartesian coordinates of connected nodes.
        
        Each graph is assigned a one-dimensional feature vector representing the object class. We sampled 3.991 graphs for training and 908 for validation.

            
        <h3>Training</h3>
            
        Batch size is set to 64; the number of message-passing layers to 2; the number of hidden features to 256; and the learning rate to $3e^{-4}$. Number of epochs is set to 50. SGD uses the ADAM optimiser. Parameter total is 923 K (Method A) and 1.1 M (Method B).
        
        
        <h3>Results</h3>

        <figure id="fig-rep-sta-2" class="auto-number-figure">
            <img src="assets/rep-sta-2.png"  style="width:400px;">
            <figcaption>
                Training performance comparison. Method A with edge attributes $\mathbf{a}_{ij} = \emptyset$, $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$, and $\mathbf{a}_{ij} = \mathbf{r}_{ij}$. Method B with edge attributes $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$ and node position update function $\phi_p$.
            </figcaption>
        </figure>

        Method A that does not use edge attributes ($\mathbf{a}_{ij} = \emptyset$) and therefore, it disregards any information the edges may carry, beyond connectivity. It is ignorant of the geometric structure between the nodes and its message passing scheme utilises the node feature vectors. Applying different rotations to the input graph gives the same latent representations. 
        
        Another model following the Method A has the geometric quantity of Euclidean distance injected into the edge attributes, $\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$.
        The representations learned with this rotation-invariant model do not change under the rotation of the input representations. 
        Rotation invariance stems from the norm in the edge attribute definition being invariant to rotations. 
        
        Last model following the Method A has relative Cartesian coordinates as attributes, $\mathbf{a}_{ij} = \mathbf{r}_{ij}$. Here, latent representations show instability under rotation. The definition of edge attribute here as the distance between two nodes, $\mathbf{r}_{ij}$, does guarantee translation but not rotation $G$-mapping. See <a href="#fig-rep-sta-1">Figure</a> for a visualisation.
        
        <figure id="fig-rep-sta-1" class="auto-number-figure">
            <img src="assets/rep-sta-1.png" width="600">
            <figcaption>
                Hidden representations of the model before pooling, using relative Cartesian coordinates as edge attributes. The disk radius reflects the first node feature value. For the input representation, 1,500 points per 3D model randomly sampled from the embedding space $\mathbb{R}^3$. Two scenarios are shown: (left) with an applied rotation of $ \theta = 0^{\circ} $ and (right) with $ \theta = 180^{\circ} $. The input representation for this visualisation differs from the others in the experiment solely in the pre-processing involving random sampling.
            </figcaption>
        </figure>
        
        Next, we compare Method B with the three variants of Method A, presented in the <a href="#tab-egnn-mpnn-compar">Table</a>. Method B is outperformed only by Method A variant with relative Cartesian coordinates as edge attributes. However, in the second part of the experiment, where we apply a random rotation to the input, its accuracy decreases significantly. This is expected, since relative Cartesian coordinates are not invariant to rotation. 
        
        The other two variants of Method A (no edge attributes and Euclidean distance) fall behind all the others in the first part of the experiment. Since they do not involve information on the direction that the messages come from, the accuracy is unchanged with the transformed input; they are invariant to rotation. Finally, we observe the stability under rotation with Method B and the higher accuracy relative to the rotation-invariant models.

        <figure>
            <table id="tab-egnn-mpnn-compar">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>ACC(a)</th>
                        <th>ACC(b)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>A ($\mathbf{a}_{ij} = \emptyset$)</td>
                        <td>.6398</td>
                        <td>.6398</td>
                    </tr>
                    <tr>
                        <td>A ($\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2$)</td>
                        <td>.6861</td>
                        <td>.6861</td>
                    </tr>
                    <tr>
                        <td>A ($\mathbf{a}_{ij} = \mathbf{r}_{ij}$)</td>
                        <td>.8061</td>
                        <td>.3876</td>
                    </tr>
                    <tr>
                        <td>B ($\mathbf{a}_{ij} = \|\mathbf{r}_{ij}\|^2, \phi_p$)</td>
                        <td>.7643</td>
                        <td>.7643</td>
                    </tr>
                </tbody>
            </table>
            <figcaption>
                Comparison of feature stability under rotation across message passing model variants. ACC (a): validation accuracy; ACC (b): Random rotation test accuracy.
            </figcaption>
        </figure>

        </div>

        <div class="references-section">
           <h2>References</h2>
            <div class="citation" id="gilmer2017" data-authors="Gilmer et al." data-year="2017">
               Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl. 
               <a href="http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf">"Neural Message Passing for Quantum Chemistry"</a>. 
               Proceedings of the 34th International Conference on Machine Learning, 2017. pp. 1263-1272.
            </div>
            <div class="citation" id="satorras2021" data-authors="Satorras et al." data-year="2017">
               Victor Garcia Satorras, Emiel Hoogeboom, Max Welling 
               <a href="https://arxiv.org/abs/2102.09844">"E(n) Equivariant Graph Neural Networks"</a>. 
               CoRR, 2021.
            </div>
            <div class="citation" id="wu2015" data-authors="Wu et al." data-year="2015">
                Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao. 
                <a href="http://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298801">"3D ShapeNets: A deep representation for volumetric shapes."</a>. 
                CVPR, 2015. pp. 1912-1920.
            </div>
           <!-- ... Other citations ... -->
        </div>
        
    </div>
    </div>
    
</body>

</html>
